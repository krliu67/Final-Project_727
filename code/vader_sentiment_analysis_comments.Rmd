```{r}
library(quanteda)
library(SentimentAnalysis)
library(vader)
library(GGally)
library(magrittr)
library(dplyr)
```

```{r}
comments <- read.csv("../data/scraped_comments_by_selected_posts.csv", 
                     header = TRUE, 
                     # sep = ";", 
                     encoding = "UTF-8") %>% distinct()
```


```{r}
text_comments <- comments %>% dplyr::select(date,comment)
```


```{r}
irrelevantWords <- c("[deleted]","[removed]")

filtered_comments <- text_comments %>% filter(comment != "[deleted]" & comment != "[removed]")
```

```{r}
sentiments <- analyzeSentiment(iconv(as.character(filtered_comments$comment), to='UTF-8'))
head(sentiments)
```
WordCount = number of non-stopwords in Reddit comments 
NegativityGI = number of negative words in Reddit comments that are in the GI negative dictionary / WordCount PositivityGI = number of negative words in Reddit comments that are in the GI positive dictionary / WordCount 
SentimentGI = PositivityGI - NegativityGI = (# positive words - # negative words) / WordCount
HE, LM, QDAP are similar

When using Lexicoder, we calculate number of positive words = positive words - negated positive + negated negative number of negative words = negative words - negated negative + negated positive
```{r}
tokenized=quanteda::tokens_lookup(quanteda::tokens(filtered_comments$comment), dictionary=data_dictionary_LSD2015, exclusive=FALSE)
sentiments$LCpos <- sapply(tokenized, function(x) sum(x=='POSITIVE') - sum(x=='NEG_POSITIVE') + sum(x=='NEG_NEGATIVE'))
sentiments$LCneg <- sapply(tokenized, function(x) sum(x=='NEGATIVE') - sum(x=='NEG_NEGATIVE') + sum(x=='NEG_POSITIVE'))
sentiments$LC <- (sentiments$LCpos-sentiments$LCneg)/sentiments$WordCount
```

```{r}
vader_scores <- vader_df(filtered_comments$comment)
sentiments$Vader <- vader_scores$compound

write.csv(vader_scores,file = "data/vader_scores.csv", row.names = FALSE)
```

```{r}
# sentiments <- read.csv("data/sentiments.csv", 
#                      header = TRUE, 
#                      # sep = ";", 
#                      encoding = "UTF-8")
```

A rule-based sentiment analysis tool called Vader. Vader takes all of the lexical and grammatical features of the reddit comments and returns a continuous sentiment score between -1 (very negative) to 1 (very positive).
To see how the dictionaries compare to each other:
```{r}
# with(sentiments, ggpairs(data.frame(SentimentGI, SentimentHE, SentimentLM, 
#                                     SentimentQDAP, LC, Vader)))



sentiments_cleaned <- sentiments %>% filter(LC != Inf) %>% filter(LC != -Inf)
with(sentiments_cleaned, ggpairs(data.frame(SentimentGI, LC, Vader)))
```
On the graph up, density. and too much highly-correlated stuff so just keep one.

On the graph below, the x-axis is the time and date Reddit comments were sent, and the y-axis is the sentiment of the reddit comments. Since the reddit comments used were gathered in the time span of a few minutes, we donâ€™t expect to see any meaningful change in sentiment over time. Sentiment of reddit comments captured over days may tell a different story, especially if any good or bad news about the vaccine came out during that time frame.
```{r}
# filtered_comments %<>%
#   separate(date, c("year","month","day"), sep = "-") 
filtered_comments$date <- as.Date(filtered_comments$date)
```

```{r}
ggplot(data=data.frame(time=filtered_comments$date, sentiment=sentiments$Vader), aes(x=time, y=sentiment)) + geom_point(alpha=.1) + geom_smooth() 

```

In the plot below, the shade of the point refers to how many reddit comments are at that location.
```{r}
ggplot(sentiments, aes(x=WordCount, y=Vader)) + geom_point(alpha=.1) + geom_smooth()
```
The shaded area in the figure represents the confidence interval around the fitted line.
This suggests greater uncertainty in predicting sentiment scores at high word counts. This may be because there are fewer data points at high word counts, or the data varies more.
```{r}
write.csv(sentiments,file = "data/sentiments.csv", row.names = FALSE)
```



```{r}
# sort wordcount
sentiments_sort_by_wordcount <- sentiments %>%
  filter(WordCount != 0) %>%
  arrange(WordCount)

sentiments_sort_by_wordcount <- sentiments_sort_by_wordcount %>% 
  dplyr::select(WordCount, Vader)
```

```{r}
ggplot(sentiments_sort_by_wordcount[1:8000,], aes(x=WordCount, y=Vader)) + geom_point(alpha=.1) + geom_smooth()
```
```{r}
summary(sentiments$Vader)

t.test(sentiments$Vader)
```

```{r}
mod <- lm(Vader~WordCount,sentiments_sort_by_wordcount)
summary(mod)
```

